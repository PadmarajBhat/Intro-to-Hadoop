{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is Big Data?\n",
    "* Can be defined as the volume of data which cannot be processed on a single machine\n",
    "* Or Cluster of nodes processing each of 1TB of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3V - Volume Variety and Velocity of Big Data\n",
    "\n",
    "* Volume - indicates the size of the data. If storage is the concern then we have to pick carefully what we store.\n",
    "* Variety - indicates that the sources are different and thier formats too. Data can be semi structured or unstructured.\n",
    "* Velocity - indicates the speed at which data arrive/availed. If we dont save the data we end up discarding them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is Hadoop?\n",
    "* Hadoop is a software framework which stores the data distributedly in the nodes of a cluster and process the data through MapReduce.\n",
    "* Since data is split and is stored in different nodes, it can support large volume of data. And more nodes can be added to cluster if more data is needed.\n",
    "* MapReduce works to get the data in the node/ distributed file system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What are the components of Hadoop?\n",
    "* HDFS - Hadoop Distributed File System\n",
    "* Map Reduce\n",
    "* PIG - High level scripting language to access HDFS through MapReduce. It is SQL like scripting used on Hadoop\n",
    "* Hive - Take Standard SQL statements and converts them to MapReduce instruction to access the HDFS.\n",
    "* Impala - Takes Standard SQL statements and works directly on the HDFC withough MapReduce in between and hence much faster.\n",
    "* Sqoop - Converts relational data base and puts it into distributed database.\n",
    "* Flume - Injests the data into HDFS as and when the external system generates it online.\n",
    "* HBase - Realtime Database on the top of HDFS\n",
    "* Mahout - Open source project to create scalable machine learning algorithm and also implements popular regression, classification and clustering algorithm for Hadoop.\n",
    "* Hue - Graphical front end to the cluster. Hadoop User Experience developed by cloudera. Can be used for monitoring and as a query editor.\n",
    "* Oozie - Workflow scheduler for hadoop jobs.\n",
    "\n",
    "#### CDH - Cloudera Distribution for Hadoop\n",
    "* Provides easy installation of all the projects in Hadoop eco system. This helps to manage the interoperability of the interlinked components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HDFS:\n",
    "* File is broken into blocks with default of 64MB and stored as 1 block/cluster node.\n",
    "* Mapping of these blocks of a single(input) file called as metadata is saved in NameNode.\n",
    "* 3 copies of blocks are kept differnt nodes in cluster to handle node failures and NameNode carefully manage the replication if a node fails.\n",
    "* To Manage the namenode failure, we have 2 options\n",
    "    * Have a back up at NFS\n",
    "    * create a standby NameNode which is a backup for Active node.\n",
    "    \n",
    "* hadoop -fs : to interact with hadoop files\n",
    "    * hadoop -fs -put readme.md --> puts readme.md file from local directory into HDFS file system.\n",
    "    * hadoop -fs -get readme.md --> puts readme.md file from HDFS file system into local directory.\n",
    "    * hadoop -fs -ls --> lists all files in HDFS\n",
    "    * hadoop -fs -mkdir input_folder --> input folder creation in HDFS\n",
    "    * hadoop -fs -put readme.md input_folder\n",
    "    * hadoop -fs -ls input_folder --> list fils in the directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MapReduce\n",
    "* Mappers: programs processes parallely and generate intermediate record of the format key and value pairs\n",
    "* Shuffle: movement of intermediate recods to reducers.\n",
    "* Sort: reducers sorting the various sources of the key.\n",
    "* reducers: process one key at time and aggregates the values and generates final results.\n",
    "\n",
    "* Partitioner: uses hash code for the keys to generate the mapper - reducer mapping. The default methode of hashing can be changed.\n",
    "\n",
    "\n",
    "##### Daemons of MapReduce:\n",
    "* 7 daemons run on all node of the cluster all the time.\n",
    "* nodes in the cluster are broadly classified as data nodes and name nodes.\n",
    "* What is mapreduce job ??? It must be identification of mapper and reducer nodes done by Job Tracker with the help of NameNode info.\n",
    "* map reduce job is given to Job Tracker. Where it runs ? Ans: It is a service from Hadoop and hence runs on Hadoop VM.\n",
    "* Job tacker locates Task tracker which is available or near to data and assigns it task. When all task tracker reply, job tracker updates the status and hence client application can opt to pool job tracker status.\n",
    "* Job tracker then splits the work into mappers and reducers and shares it with nodes in the cluster.\n",
    "* can mappers and reducers co exist in a node ? Yes, but ideally not. This would increase the burden on the node and cause latency.\n",
    "* mappers and reducers tasks run by Task Tracker in each node.\n",
    "* mappers will be processing a portion of data ( duh definition of mapper) and that portion of the data is called input split. By default, input split is taken from the same node on which mapper runs.\n",
    "* Streaming: In case, the mapper is running on a different node beacause the node with input split is busy then streaming takes place between the nodes (one with mapper running and another with input split) for mapper processing.\n",
    "\n",
    "* hs mapper.py reducer.py joboutputdir --> starts the map reduce job and shows the progress\n",
    "    * completion would have 3 files in the output directory\n",
    "        * success file indicating the status of the map reduce job. Would it have any content? as in 0 for failed case? or will not be there in case of failed case?\n",
    "        * logs file\n",
    "        * part-00000 file: Contains the result of the mapreduce job.\n",
    "    * output directory should not pre exist\n",
    "        \n",
    "* Hows cluster and hadoop are configured?\n",
    "    * we can see the configuration through the command : hdfs dfsadmin -report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hadoop installation:\n",
    "* downloaded latest Oracle VM free software\n",
    "* downloaded and unzip udacity cloud virtual box\n",
    "* ran hadoop fs file related commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hadoop Hands on exercises:\n",
    "* hs mapper.py reducer.py inputdir outputdir --> this command will start the mapper and reducer job \n",
    "* if the output directory is present already it fails\n",
    "* output directory had 3 files:\n",
    "    * \\_SUCCESS to indicate the outcome of the folder and was of size 0 bytes\n",
    "    * \\_logs directory had 2 files in it some configuration and jar file\n",
    "    * part-0000 with the output\n",
    "    \n",
    "* it has to be noted that mapper completing 80% only trigger reducer. There may be couple of reasons for that. \n",
    "    * Mapper mapped to reducer did not complete\n",
    "    * mapper and reducer were on same node because training VM.\n",
    "    \n",
    "* How do we see how many clusters are there ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hadoop Docs:\n",
    "\n",
    "* Lesson Learnt in Hadoop: https://www.cio.com/article/3100670/five-important-lessons-learned-from-a-journey-to-hadoop.html\n",
    "* Hadoop Hacks: https://www.kdnuggets.com/jobs/15/11-11-inventure-data-analyst.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hadoop Best Practises:\n",
    "* test the mapper and reduce code first with a small program and then run it on entire code. you may use the pipe facility in the unix to simulate the map reduce shuffle and sort\n",
    "* cat test1 |  mapper.py | sort | reducer.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Map Reduce Design Patterns:\n",
    "* Filtering Patterns: Sampling, Top-N\n",
    "* Summarization Pattern: Counting, Min/Max, Statistics, Index\n",
    "* Structural Patterns: Combining Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark:\n",
    "##### NPTEL:\n",
    "* Map Reduce: Has latency issue because of mapper results being stored in HDFS and then shared with reducers.\n",
    "* This is considerably slow for iterative and interactive applications.\n",
    "* for the same BSP - bulk synchronous processing and iterative map reduce where introduced.\n",
    "* Later Spark came into picture with RDD ( Risilent Distribted Datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* e.g. : Spark code in Python\n",
    "\n",
    "import sys\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc= SparkContext(\"local\", \"wordcount\", sys.argv[0], None)\n",
    "\n",
    "lines = sc.textFile(sys.argv[1])\n",
    "\n",
    "lines.flatMap(lambda s: s.split(\" \")) \\\n",
    "        .map(lambda word: (word,1)) \\\n",
    "        .reduceByKey(lambda x,y : x+y) \\\n",
    "        .saveAsTextFile(sys.argv[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Resilent Distributed Datasets (RDDs)\n",
    "    * immutable and partitioned collection of records\n",
    "    * they are built through transformation like map, join etc\n",
    "    * they reside in cache for efficient reuse.\n",
    "    * Fault tolerance is through a technique called lineage: recomputes the lost partition through the necessary transformation.\n",
    "    * HDFS --> RDD -->Map-->(stored in )RDD --> Reduce --> (stored in) RDD --> Result are also stored in RDD and cached for  low latency retrieval.\n",
    "    * Since intermediate results are saved in RDD, Lineage simply recomputes to recover the lost data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark - Edureka\n",
    "* Spark Context is the entry point for the spark programming \n",
    "\n",
    "* textFile will load the data from the HDFS to the memory. In the above example, lines holds data in memory and post loading into memory it is termed as RDD\n",
    "\n",
    "* though it resides on the volatime memory, the fault tolerance is handled through replication and hence respective nodes loads it in its memory thus maintaining \"resilence\" - reliable\n",
    "\n",
    "* MLLib is much faster than Mahout which is MapReduce based. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
