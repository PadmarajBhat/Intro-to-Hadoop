{"cells": [{"metadata": {"trusted": false}, "cell_type": "code", "source": "print(\"Spark session is Loaded !!!\")\nsdf = spark.read.csv('hdfs://ec2-18-223-23-172.us-east-2.compute.amazonaws.com:8020/user/root/temp.csv')\nfor i in range(0,2500):\n    #sdf = sdf.union(sdf)\n    #sdf.write.csv(path=\"large_temp_2.csv\")\n    sdf.write.save(path=\"large_temp_2.csv\", format='csv', mode='append')", "execution_count": null, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5fb40c6ba83a470baf9fa84e3c86cc4a"}}, "metadata": {}}]}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "new_sdf = spark.read.csv(\"large_temp_2.csv\",header=True,encoding='utf-8')\nnew_sdf.count()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "##### New approach:"}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "def doubleIT():\n    sdf = spark.read.csv(\"large_temp_2.csv\")\n    sdf.write.save(path=\"large_temp_2.csv\", format='csv', mode='append')\n    \nfor i in range(0,10):\n    doubleIT()\n    \n    \nsdf = spark.read.csv(\"large_temp_2.csv\")\nsdf.count()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "* How to know the large_temp_2.csv FOLDER size?\n    * **Ans**: hadoop fs -du -s -h /user/livy/large_temp_2.csv\n\t\t* 34.2 M  /user/livy/large_temp_2.csv\n            * from the new approach of doubleIT, it should reach a TB atleast\n    *  hdfs dfs -df -h\n    ```\n        Filesystem                                                  Size     Used  Available  Use%\n        hdfs://ip-172-31-39-242.us-east-2.compute.internal:8020  497.8 G  182.5 G    297.6 G   37%\n    ```\n\n* can 2 jobs, one for append and one for count work in parallel ? \n    * when I tried 2 spark submit (from 2 cells) in HUE notebook, both spark job started together but job with \"count\" stopped till \"append\" job completed.\n    \n    \n* what is time to read and count for csv file?\n* what is time to read and count for parquete file?\n* can panda hold the csv or parquet file ?\n    * if yes, can it operate on a \"query\" with the same speed as that of spark dataframe?\n        * obviously not, otherwise spark should have closed its shutter !!! but let us see the difference and hail spark\n    * if no, can we slice the dataset using spark df and may be panda simply does the plot( then why not matplot lib?)\n"}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": ""}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 2}, "pygments_lexer": "python2"}}, "nbformat": 4, "nbformat_minor": 2}