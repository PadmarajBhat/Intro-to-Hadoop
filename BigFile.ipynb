{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "print(\"Spark session is Loaded !!!\")\nsdf = spark.read.csv('hdfs://ec2-18-223-23-172.us-east-2.compute.amazonaws.com:8020/user/root/temp.csv')\nfor i in range(0,2500):\n    #sdf = sdf.union(sdf)\n    #sdf.write.csv(path=\"large_temp_2.csv\")\n    sdf.write.save(path=\"large_temp_2.csv\", format='csv', mode='append')", "execution_count": null, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5fb40c6ba83a470baf9fa84e3c86cc4a"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "new_sdf = spark.read.csv(\"large_temp_2.csv\",header=True,encoding='utf-8')\nnew_sdf.count()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "##### New approach:"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def doubleIT():\n    sdf = spark.read.csv(\"large_temp_2.csv\")\n    sdf.write.save(path=\"large_temp_2.csv\", format='csv', mode='append')\n    \nfor i in range(0,10):\n    doubleIT()\n    \n    \nsdf = spark.read.csv(\"large_temp_2.csv\")\nsdf.count()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "* How to know the large_temp_2.csv FOLDER size?\n    * **Ans**: hadoop fs -du -s -h /user/livy/large_temp_2.csv\n\t\t* 34.2 M  /user/livy/large_temp_2.csv\n            * from the new approach of doubleIT, it should reach a TB atleast\n                * I had to stop the doubleIT execution as it was approaching 75% hadoop capacity but i think system did rollback of some last right because of which it came back 57% hdfs usage.\n                \n    *  hdfs dfs -df -h\n    ```\n        Filesystem                                                  Size     Used  Available  Use%\n        hdfs://ip-172-31-39-242.us-east-2.compute.internal:8020  497.8 G  182.5 G    297.6 G   37%\n    ```\n\n* can 2 jobs, one for append and one for count work in parallel ? \n    * when I tried 2 spark submit (from 2 cells) in HUE notebook, both spark job started together but job with \"count\" stopped till \"append\" job completed. I could not find any uncommitted read option.\n    \n    \n* what is time to read and count for csv file?\n    * It did not read the file completely even if the spark job ran for 15 min.\n    * I requested for 100 instances but that instance request got queued up and did not complete. I am wondering why the instance request got queued up. May be the resource manager , yarn, was busy scheduling the heavy IO operation for csv read.\n    * yarn, in its last few logs in the HUE notebook cell output indicated the number of executors\n    ```\n    YARN Diagnostics:\n19/05/02 20:03:00 INFO ExecutorAllocationManager: Requesting 3217 new executors because tasks are backlogged (new desired total will be 7312)\n\n    ```\n        * Because of this huge demand, abrupt closure resulted in huge rollback.\n        * **What is the relation between instances and executors?** I think initial spark job log talks about it. \n        ```\n        19/05/03 04:26:13 INFO RMProxy: Connecting to ResourceManager at ip-172-31-39-242.us-east-2.compute.internal/172.31.39.242:8032\n        19/05/03 04:26:13 INFO Client: Requesting a new application from cluster with 46 NodeManagers\n        19/05/03 04:26:13 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (11520 MB per container)\n        19/05/03 04:26:13 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n        19/05/03 04:26:13 INFO Client: Setting up container launch context for our AM\n        19/05/03 04:26:13 INFO Client: Setting up the launch environment for our AM container\n        19/05/03 04:26:13 INFO Client: Preparing resources for our AM container\n        .\n        .\n        19/05/03 04:27:15 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 7312 output partitions\n        .\n        .\n        19/05/03 04:27:16 INFO DAGScheduler: Submitting 7312 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n19/05/03 04:27:16 INFO YarnScheduler: Adding task set 0.0 with 7312 tasks\n        .\n        .\n        .\n        19/05/03 04:27:34 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1948 ms on ip-172-31-40-6.us-east-2.compute.internal (executor 175) (1/7312)\n\n        ```\n        \n        \n        * Here 44 nodes are from the \"spot\" booked task nodes and 2 core nodes \n            * core nodes are presistent nodes and are created when EMR is iniialized. The also seem to define the storage capacity for the HDFS because \n                * if I try stop it, AWS throws warning indicating loss of data stored. \n                * and also \"hdfs dfs -df -h\" seems to sum up the storage capacity of core nodes.\n                \n        * Here, number of tasks are directly proportional to ouput partitions and it is difficult to map instances with tasks becuase completion of the previous task and the task itself may vary within the instance.\n\n\n* How long did write to ORC file took?\n    * 26 minutes and file size got reduced to 1.1 GB. Note that CSV file was around 300GB.\n    * It took only 26 seconds to read the file and get the count ...wow !!! \n    * Now the question is ...Are we doing apple to apple comparison?\n        * i.e. would csv file of 1.1 gb size can read and count in 26 seconds. It is understood that record count will be different but does the read capacity change? My intuition says yes csv file can read in the same speed as task executor of similar capacity. Raw fetch time should remains the same. \n            * Indeed my experiments wih 1.1GB csv data read and count showd fantastic result : 3 secs of execution time. But records count where drastically down.\n\n\n* what is time to read and count for parquete file?\n    * I have to know how to rename the columns in spark dataframe???\n        ```\n        par_sdf = spark.read.orc(\"large_temp_2_avro\")\n        old_columns = par_sdf.columns\n        new_columns = []\n        for c in old_columns:\n            new_columns.append(c.lower().replace(\" \",\"_\").replace(\"'\",\"\"))\n\n        print(new_columns)\n\n        par_new_df = reduce(lambda data, idx: data.withColumnRenamed(old_columns[idx], new_columns[idx]) , xrange(len(old_columns)), par_sdf)\n        par_new_df.show()\n        par_new_df.write.parquet(\"large_temp_parquet\")\n        ```\n        * Above piece of code converted the buggy colulmn name saved in ORC file and saves the dataframe (from ORC read) to parquet file format.\n        * Parquet file Write operation took total of 20 minutes. File size is same as that of ORC = 1.1 GB\n        * Read and count took mere 14 seconds\n    \n    \n    \n* can panda hold the csv or parquet file ?\n    * if yes, can it operate on a \"query\" with the same speed as that of spark dataframe?\n        * obviously not, otherwise spark should have closed its shutter !!! but let us see the difference and hail spark\n    * if no, can we slice the dataset using spark df and may be panda simply does the plot( then why not matplot lib?)\n\n"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": ""}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 2}, "pygments_lexer": "python2"}}, "nbformat": 4, "nbformat_minor": 2}